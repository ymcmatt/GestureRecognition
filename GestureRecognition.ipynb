{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkTemplate(curr):\n",
    "    # check if the frame matches the template for static gestures: paper, rock and scissor\n",
    "\n",
    "    # store the max_val of template matching to \"scores\" and max_loc to \"max_l\"\n",
    "    myTemplate = ['rock.jpg','scissor.jpg','paper.jpg']\n",
    "    scores={\"rock\":0,\"scissor\":0,\"paper\":0}\n",
    "    max_l = {\"rock\":0,\"scissor\":0,\"paper\":0}\n",
    "\n",
    "    # run the frame to match all three possible cases and choose the most probable one\n",
    "    for i in range(3):\n",
    "        template = cv2.imread(myTemplate[i])\n",
    "        template = mySkinDetect(template)\n",
    "        tempw, temph = template.shape[:2]\n",
    "\n",
    "        img = curr\n",
    "        imgw, imgh = img.shape[:2]\n",
    "        img = mySkinDetect(img)\n",
    "        if (tempw > imgw or temph > imgh):\n",
    "            img = cv2.resize(img,(round(tempw*1.5),round(temph*1.5)))\n",
    "\n",
    "        w,h = template.shape[:2]\n",
    "\n",
    "        res = cv2.matchTemplate(img,template,cv2.TM_CCOEFF_NORMED)\n",
    "        min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)\n",
    "        scores[myTemplate[i][:-4]]=max_val\n",
    "        max_l[myTemplate[i][:-4]]=max_loc\n",
    "    \n",
    "    decision=max(scores,key=scores.get)\n",
    "    score=max(scores)\n",
    "        \n",
    "    max_loc = max_l[decision]\n",
    "    \n",
    "    print(\"decision\",decision)    \n",
    "\n",
    "    #scale the rectangle with respect to updated image size (150,150)\n",
    "    x,y = img.shape[:2]\n",
    "    scaleX = x // 150\n",
    "    scaleY = y // 150\n",
    "\n",
    "    top_left = (max_loc[0] // scaleX, max_loc[1] // scaleY)\n",
    "    bottom_right = (top_left[0]+ w//scaleY ,top_left[1] + h//scaleX)\n",
    "    \n",
    "    #scale the img size to (150,150)\n",
    "    img = cv2.resize(img,(150,150))\n",
    "    \n",
    "    # Draw rectangle on the image\n",
    "    cv2.rectangle(img,top_left,bottom_right,(255,0,0),2)\n",
    "    \n",
    "    # write the decision text on the image\n",
    "    cv2.putText(img,decision, (25,25), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    cv2.imwrite(\"res.jpg\",img)\n",
    "    cv2.imshow(\"result\",img)\n",
    "            \n",
    "    return (score,decision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkTemplateD(curr):\n",
    "\n",
    "    # check if the frame matches the template for dynamic gestures: waving or non-waving\n",
    "    \n",
    "    # store the max_val of template matching to \"scores\" and max_loc to \"max_l\"\n",
    "    myTemplate = [\"waving_blob.png\",\"non_wave_blob.png\"]\n",
    "    scores={'waving':0,'non-wave':0}\n",
    "    max_l = {'waving':0,'non-wave':0}\n",
    "    \n",
    "    # run the frame to match both possible cases and choose the most probable one\n",
    "    for i in range(2):\n",
    "        template = cv2.imread(myTemplate[i])\n",
    "        tempw, temph = template.shape[:2]\n",
    "\n",
    "        img = curr\n",
    "        imgw, imgh = img.shape[:2]\n",
    "\n",
    "        if (tempw >= imgw or temph >= imgh):\n",
    "            img = cv2.resize(img,(round(tempw*2),round(temph*2)))\n",
    "\n",
    "        w,h = template.shape[:2]\n",
    "\n",
    "        res = cv2.matchTemplate(img,template,cv2.TM_CCOEFF_NORMED)\n",
    "        min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)\n",
    "        scores[myTemplate[i][:-4]]=max_val\n",
    "        max_l[myTemplate[i][:-4]]=max_loc\n",
    "        \n",
    "    decision=max(scores,key=scores.get)\n",
    "    score=max(scores)\n",
    "\n",
    "    #scale the rectangle with respect to updated image size (150,150)\n",
    "    x,y = img.shape[:2]\n",
    "    scaleX = x // 150\n",
    "    scaleY = y // 150\n",
    "    max_loc = max_l[decision]\n",
    "\n",
    "    top_left = (max_loc[0] // scaleX, max_loc[1] // scaleY)\n",
    "    bottom_right = (top_left[0]+ w//scaleY ,top_left[1] + h//scaleX)\n",
    "    \n",
    "    #scale the img size to (150,150)\n",
    "    img = cv2.resize(img,(150,150))\n",
    "    \n",
    "    # Draw rectangle on the image\n",
    "    cv2.rectangle(img,top_left,bottom_right,(0,0,255),2)\n",
    "    \n",
    "    # write the decision text on the image\n",
    "    cv2.putText(img,decision, (25,25), cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "    cv2.imwrite(\"res.jpg\",img)\n",
    "    cv2.imshow(\"result\",img)     \n",
    "\n",
    "    return (score,decision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that detects whether a pixel belongs to the skin based on RGB values\n",
    "# src - the source color image\n",
    "# dst - the destination grayscale image where skin pixels are colored white and the rest are colored black\n",
    "def mySkinDetect(src):\n",
    "    # Surveys of skin color modeling and detection techniques:\n",
    "    # 1. Vezhnevets, Vladimir, Vassili Sazonov, and Alla Andreeva. \"A survey on pixel-based skin color detection techniques.\" Proc. Graphicon. Vol. 3. 2003.\n",
    "    # 2. Kakumanu, Praveen, Sokratis Makrogiannis, and Nikolaos Bourbakis. \"A survey of skin-color modeling and detection methods.\" Pattern recognition 40.3 (2007): 1106-1122.\n",
    "    dst = np.zeros((src.shape[0], src.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(src.shape[0]):\n",
    "        for j in range(src.shape[1]):\n",
    "            #b,g,r = src[i,j]\n",
    "            b = int(src[i,j][0])\n",
    "            g = int(src[i,j][1])\n",
    "            r = int(src[i,j][2])\n",
    "            if(r>95 and g>40 and b>20 and max(r,g,b)-min(r,g,b)>15 and abs(r-g)>15 and r>g and r>b):\n",
    "                dst[i,j] = 255\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that does frame differencing between the current frame and the previous frame\n",
    "# prev - the previous color image\n",
    "# curr - the current color image\n",
    "# dst - the destination grayscale image where pixels are colored white if the corresponding pixel intensities in the current\n",
    "# and previous image are not the same\n",
    "def myFrameDifferencing(prev, curr):\n",
    "    # For more information on operation with arrays: \n",
    "    # http://docs.opencv.org/modules/core/doc/operations_on_arrays.html\n",
    "    \n",
    "    # code here...\n",
    "    dst = cv2.absdiff(prev,curr)\n",
    "    dst = cv2.cvtColor(dst,cv2.COLOR_BGR2GRAY)\n",
    "    _, dst = cv2.threshold(dst, 50, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #dst = np.zeros((prev.shape[0], prev.shape[1], 1), dtype = \"uint8\")\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that accumulates the frame differences for a certain number of pairs of frames\n",
    "# mh - vector of frame difference images\n",
    "# dst - the destination grayscale image to store the accumulation of the frame difference images\n",
    "def myMotionEnergy(mh):\n",
    "    # the window of time is 3\n",
    "    mh0 = mh[0]\n",
    "    mh1 = mh[1]\n",
    "    mh2 = mh[2]\n",
    "    mh3 = mh[3]\n",
    "    mh4 = mh[4]\n",
    "    dst = np.zeros((mh0.shape[0], mh0.shape[1], 1), dtype = \"uint8\")\n",
    "    \n",
    "    # code here...\n",
    "    for i in range(mh0.shape[0]):\n",
    "        for j in range(mh0.shape[1]):\n",
    "            if mh0[i,j]==255 or mh1[i,j]==255 or mh2[i,j] == 255 or mh3[i,j] == 255 or mh4[i,j] == 255:\n",
    "                dst[i,j] = 255\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # a) Reading a stream of images from a webcamera, and displaying the video\n",
    "    # open the video camera no. 0\n",
    "    # for more information on reading and writing video: http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open the video cam\")\n",
    "        return -1\n",
    "    \n",
    "    y_true = ['paper','scissor','paper','paper','scissor','rock','paper','rock','scissor','paper','scissor','rock','rock','paper','paper','rock','rock','scissor','paper','paper']\n",
    "    y_pred = ['rock','scissor','paper','paper','scissor','rock','paper','rock','scissor','paper','scissor','rock','rock','rock','rock','rock','rock','rock','rock','paper']\n",
    "    \n",
    "    print(confusion_matrix(y_true,y_pred,labels=['rock','paper','scissor']))\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    \n",
    "    # read a new frame from video\n",
    "    success, prev_frame = cap.read()\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not success:\n",
    "        print(\"Cannot read a frame from video stream\")\n",
    "        return -1\n",
    "    cv2.namedWindow(\"frame\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    prev_frame = cv2.resize(prev_frame,(150,150))\n",
    "    fMH1 = np.zeros((prev_frame.shape[0], prev_frame.shape[1], 1), dtype = \"uint8\")\n",
    "    fMH2 = fMH1.copy()\n",
    "    fMH3 = fMH1.copy()\n",
    "    fMH4 = fMH1.copy()\n",
    "    fMH5 = fMH1.copy()\n",
    "    myMotionHistory = deque([fMH1, fMH2, fMH3,fMH4,fMH5]) \n",
    "    frame = 0\n",
    "    while(True):\n",
    "        #read a new frame from video\n",
    "        \n",
    "        success, curr_frame = cap.read()\n",
    "        curr_frame = cv2.resize(curr_frame,(150,150))\n",
    "        frame += 1\n",
    "            \n",
    "        \n",
    "        if not success:\n",
    "            print(\"Cannot read a frame from video stream\")\n",
    "            break\n",
    "    \n",
    "        cv2.imshow('frame',curr_frame)\n",
    "\n",
    "        # b) Skin color detection\n",
    "        mySkin = mySkinDetect(curr_frame)\n",
    "        cv2.imshow('mySkinDetect',mySkin)\n",
    "\n",
    "        # c) Background differencing\n",
    "        frameDest = myFrameDifferencing(prev_frame, curr_frame)\n",
    "        cv2.imshow('myFrameDifferencing',frameDest)\n",
    "\n",
    "        # d) Visualizing motion history\n",
    "        myMotionHistory.popleft()\n",
    "        myMotionHistory.append(frameDest)\n",
    "        myMH = myMotionEnergy(myMotionHistory)\n",
    "        cv2.imshow('myMotionHistory',myMH)\n",
    "        \n",
    "        #score1,decision1 = checkTemplate(curr_frame)\n",
    "        if frame% 25 == 0:\n",
    "            cv2.imwrite('motion.jpg',myMH)\n",
    "            frame_captured=cv2.imread('motion.jpg')\n",
    "            frame_captured = cv2.resize(frame_captured,(150,150))\n",
    "            cv2.imshow('frame.jpg',frame_captured)\n",
    "            #checkTemplateD(frame_captured)\n",
    "            checkTemplate(curr_frame)\n",
    "\n",
    "        prev_frame = curr_frame\n",
    "        \n",
    "        # wait for 'q' key press. If 'q' key is pressed, break loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 0 0]\n",
      " [4 5 0]\n",
      " [1 0 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       paper       1.00      0.56      0.71         9\n",
      "        rock       0.55      1.00      0.71         6\n",
      "     scissor       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        20\n",
      "   macro avg       0.85      0.79      0.77        20\n",
      "weighted avg       0.86      0.75      0.76        20\n",
      "\n",
      "decision paper\n",
      "decision rock\n",
      "decision rock\n",
      "decision rock\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
